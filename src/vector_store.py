# src/vector_store.py

import os
import weaviate
import weaviate.classes as wvc
import ollama
from langchain_ollama import OllamaEmbeddings
from . import config

class VectorStore:
    """
    A class to manage all interactions with the Weaviate vector store.
    """
    def __init__(self):
        """
        The constructor for the VectorStore class.
        It automatically handles connecting to Weaviate and checking Ollama.
        """
        self.client = None
        print("--- Initializing VectorStore ---")
        self._connect_to_weaviate()
        self._check_ollama_connection()

    def _connect_to_weaviate(self):
        # This method is perfect as is. No changes needed.
        print("--> Attempting to connect to Weaviate...")
        wcs_url = os.getenv("WEAVIATE_URL")
        wcs_api_key = os.getenv("WEAVIATE_API_KEY")

        if not wcs_url or not wcs_api_key:
            raise ValueError("WEAVIATE_URL and WEAVIATE_API_KEY must be set in your .env file")

        try:
            client = weaviate.connect_to_weaviate_cloud(
                cluster_url=wcs_url,
                auth_credentials=weaviate.auth.AuthApiKey(wcs_api_key),
                skip_init_checks=True
            )
            self.client = client
            print("âœ… Successfully connected to Weaviate.")
        except Exception as e:
            print(f"âŒ FAILED to connect to Weaviate: {e}")
            raise

    def _check_ollama_connection(self):
        # This method is perfect as is. No changes needed.
        print("--> Checking connection to Ollama...")
        try:
            ollama.list()
            print("âœ… Successfully connected to Ollama.")
        except Exception:
            print("âŒ FAILED to connect to Ollama. Please ensure the Ollama application is running.")
            raise

    def ingest_data(self, chunks: list[dict]):
        """
        Embeds and ingests data chunks with metadata into the Weaviate collection.
        """
        print("\n--- Starting Data Ingestion into Weaviate ---")
        
        # Initialize the embedding model from Ollama
        embeddings = OllamaEmbeddings(model=config.OLLAMA_EMBEDDING_MODEL)
        
        # Get the collection name from our config file
        collection_name = config.COLLECTION_NAME

        # For development, it's good practice to start fresh.
        if self.client.collections.exists(collection_name):
            print(f"- Deleting existing collection '{collection_name}' to ensure a clean slate.")
            self.client.collections.delete(collection_name)

        # Define the structure of our data collection in Weaviate
        print(f"- Creating new collection: '{collection_name}'")
        collection = self.client.collections.create(
            name=collection_name,
            properties=[
                # This property will store the raw text of the chunk
                wvc.config.Property(name="content", data_type=wvc.config.DataType.TEXT),
                # This property will store our metadata (e.g., "Page 54")
                wvc.config.Property(name="source", data_type=wvc.config.DataType.TEXT),
            ],
            # We must tell Weaviate not to use its own vectorizer, since we are providing our own
            vectorizer_config=wvc.config.Configure.Vectorizer.none(),
        )
        
        # Use batching for efficient data insertion
        print("- Preparing to ingest data in batches...")
        with collection.batch.dynamic() as batch:
            for i, chunk in enumerate(chunks):
                print(f"- Processing chunk {i+1}/{len(chunks)} from {chunk['source']}...")
                
                # The data object to be stored, including our metadata
                data_object = {
                    "content": chunk['text'],
                    "source": chunk['source'],
                }
                
                # Generate the vector embedding for the text content
                vector = embeddings.embed_query(chunk['text'])
                
                # Add the complete object (data + vector) to the batch
                batch.add_object(
                    properties=data_object,
                    vector=vector
                )
        
        print("âœ… Data ingestion complete!")

    def query(self, query_text: str, top_k: int = 1) -> list[dict]:
        """
        Queries the vector store for the most relevant chunks for a given text.
        Returns a list of chunk objects, each containing 'content' and 'source'.
        """
        print(f"\nðŸ”Ž Searching for context relevant to: '{query_text}'...")
        
        # We need the embedding model again to convert the query text to a vector
        embeddings = OllamaEmbeddings(model=config.OLLAMA_EMBEDDING_MODEL)
        query_vector = embeddings.embed_query(query_text)
        
        # Get the collection object
        collection = self.client.collections.get(config.COLLECTION_NAME)
        
        # Perform the vector search
        response = collection.query.near_vector(
            near_vector=query_vector,
            limit=config.TOP_K,
            # Crucially, we ask it to return the properties we stored
            return_properties=["content", "source"] 
        )
        
        retrieved_chunks = [item.properties for item in response.objects]
            
        print(f"âœ… Found {len(retrieved_chunks)} relevant chunks.")
        return retrieved_chunks

    def close(self):
        """Closes the Weaviate client connection if it exists."""
        if self.client:
            self.client.close()
            print("\nâœ… Connection to Weaviate closed properly.")